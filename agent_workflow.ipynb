{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc4c55f",
   "metadata": {},
   "source": [
    "# Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, MultiPolygon, GeometryCollection\n",
    "\n",
    "from utils import cut_polygon\n",
    "from agent_toolkit import (\n",
    "    AgentOrchestrator,\n",
    "    ReasoningAgent,\n",
    "    ValidationAgent,\n",
    "    faiss_search,\n",
    "    faiss_search_flatten,\n",
    "    gpkg_query,\n",
    ")\n",
    "from agent_prompts import SYSPROMPT_REASONING_AGENT, SYSPROMPT_VALIDATION_AGENT\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5eb3d2",
   "metadata": {},
   "source": [
    "### loading RAG and GEO databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a600fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FAISS database\n",
    "EMB_DIM = 3072\n",
    "EMBEDDER = \"text-embedding-3-large\"\n",
    "\n",
    "FAISS_INDEX_DIR = \"data/spatial_genai_storage/database_RAG\"\n",
    "FAISS_META_PATH = Path(FAISS_INDEX_DIR) / \"metadata.json\"\n",
    "FAISS_INDEX_PATH = str(Path(FAISS_INDEX_DIR) / \"faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load geopackage data\n",
    "\n",
    "GPKG_FILE_PATH = 'data/spatial_genai_storage/data_PDOK/top10nl_Compleet.gpkg'\n",
    "BBOX_UTRECHT_PROV = (109311, 430032, 169326, 479261)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac61ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the province of Utrecht polygon as base polygon\n",
    "base_polygon = gpkg_query(\n",
    "    table_name='top10nl_registratief_gebied_vlak',\n",
    "    bbox=BBOX_UTRECHT_PROV,\n",
    "    filters={'typeregistratiefgebied': 'provincie', 'naamnl': 'Utrecht'}  # Adjust column if name is in 'naamofficieel'\n",
    ")\n",
    "base_geom = base_polygon.geometry.iloc[0]  # Extract Shapely geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d7abe",
   "metadata": {},
   "source": [
    "### Setting up API calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce339ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f6334a",
   "metadata": {},
   "source": [
    "### Basic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd33978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column:feature filters plus reasoning\n",
    "NUM_ROUNDS = 2\n",
    "\n",
    "# basic inputs\n",
    "thematic_object = \"windturbine\"\n",
    "object_description = \"Er is uitgegaan van een 'referentie windturbinetype'. Uitgangspunt is dat het referentie windturbinetype een ashoogte van 160 meter, rotordiameter van 162 meter en tiphoogte van 241 meter heeft.\"\n",
    "base_polygon_name = \"Provincie Utrecht\"\n",
    "\n",
    "# get judicial references from RAG database\n",
    "query = f\"Vind juridische belemmeringen voor het plaatsen van een {thematic_object} met de volgende kenmerken: {object_description}. Geef relevante wet- en regelgeving, beleidsdocumenten en ruimtelijke plannen binnen {base_polygon_name}.\"\n",
    "judicial_reference = faiss_search(query=query, k=30, location=\"Utrecht\")\n",
    "judicial_reference = faiss_search_flatten(judicial_reference)\n",
    "\n",
    "# get database reference from pre-made file\n",
    "database_reference_path = \"data/spatial_genai_storage/data_PDOK/llm_reference.txt\"\n",
    "with open(database_reference_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    database_reference = f.read()\n",
    "\n",
    "# prepare agent ecosystem\n",
    "reasoning_agent = ReasoningAgent(\n",
    "    name=\"inhoudsexpert\",\n",
    "    system_prompt_template=SYSPROMPT_REASONING_AGENT,\n",
    ")\n",
    "validation_agent = ValidationAgent(\n",
    "    name=\"kwaliteitscontroleur\",\n",
    "    system_prompt_template=SYSPROMPT_VALIDATION_AGENT,\n",
    ")\n",
    "orchestrator = AgentOrchestrator(agents=[reasoning_agent, validation_agent])\n",
    "\n",
    "# context replacement happens through agent_toolkit.prepare_system_prompt()\n",
    "agent_context = {\n",
    "    \"thematic_object\": thematic_object,\n",
    "    \"object_description\": object_description,\n",
    "    \"judicial_reference\": judicial_reference,\n",
    "    \"database_reference\": database_reference,\n",
    "    \"base_polygon_name\": base_polygon_name,\n",
    "    \"feedback\": [],\n",
    "}\n",
    "\n",
    "agent_rounds = orchestrator.run_rounds(agent_context, num_rounds=NUM_ROUNDS)\n",
    "latest_round = agent_rounds[-1]\n",
    "filters_plus_reasoning = latest_round[\"reasoning\"][\"raw\"]\n",
    "filters = latest_round[\"reasoning\"][\"parsed\"]\n",
    "validation_summary = latest_round[\"validation\"][\"parsed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect validation feedback from the latest round\n",
    "validation_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filters_plus_reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be563c",
   "metadata": {},
   "source": [
    "### Start processing (takes a while, 15-20min... with bigger boundary box 35-45min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4410ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut from province polygon using json format filters\n",
    "\n",
    "categories = {\n",
    "    \"harde belemmering\": [],\n",
    "    \"complexe belemmering\": [],\n",
    "    \"zachte belemmering\": []\n",
    "}\n",
    "\n",
    "for item in filters:\n",
    "    categorie = item.get(\"categorie\", \"\").lower()\n",
    "    if categorie in categories:\n",
    "        categories[categorie].append(item)\n",
    "        \n",
    "# Store cut geometries per category\n",
    "cut_areas_by_category = {cat: [] for cat in categories.keys()}\n",
    "\n",
    "# Process all filters and apply to base geometry\n",
    "for item in filters:\n",
    "    tabel = item[\"tabel\"]\n",
    "    kolom = item[\"kolom\"]\n",
    "    waarde = item[\"waarde\"]\n",
    "    categorie = item.get(\"categorie\", \"\").lower()\n",
    "\n",
    "    print(f\"Processing filter: {tabel}.{kolom} = '{waarde}'\")\n",
    "    cut_gdf = gpkg_query(table_name=tabel, filters={kolom: waarde})\n",
    "    \n",
    "    if not cut_gdf.empty:\n",
    "        cut_geoms = cut_gdf.geometry.tolist()\n",
    "        union_cut = unary_union(cut_geoms)\n",
    "        \n",
    "        if categorie in cut_areas_by_category:\n",
    "            cut_areas_by_category[categorie].append(union_cut)\n",
    "        \n",
    "        base_geom = cut_polygon(base_geom, union_cut)\n",
    "        \n",
    "        print(f\"Cut applied for {tabel}.{kolom}='{waarde}' - {len(cut_gdf)} polygons removed.\")\n",
    "    else:\n",
    "        print(f\"No polygons found for {tabel}.{kolom}='{waarde}' - skipping.\")\n",
    "\n",
    "# Create new GeoDataFrame with cut geometry\n",
    "cut_gdf_result = gpd.GeoDataFrame(\n",
    "    {\"name\": [\"Utrecht_Province_Cut\"], \"description\": [\"Remaining area after all cuts\"]},\n",
    "    geometry=[base_geom],\n",
    "    crs=base_polygon.crs\n",
    ")\n",
    "\n",
    "# Optional: Save to file\n",
    "output_file = \"utrecht_cut_with_categories.gpkg\"\n",
    "cut_gdf_result.to_file(output_file, layer=\"remaining_area\", driver=\"GPKG\")\n",
    "print(f\"Remaining area saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e84c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"utrecht_cut_with_categories.gpkg\"\n",
    "cut_gdf_result.to_file(output_file, layer=\"remaining_area\", driver=\"GPKG\")\n",
    "print(f\"Remaining area saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cut areas by category as separate layers\n",
    "for categorie, cut_geoms in cut_areas_by_category.items():\n",
    "    if not cut_geoms:\n",
    "        print(f\"No cut areas for category: {categorie}\")\n",
    "        continue\n",
    "    \n",
    "    # Union all cut geometries for this category\n",
    "    category_union = unary_union(cut_geoms)\n",
    "    \n",
    "    if category_union.is_empty:\n",
    "        print(f\"Empty geometry for category: {categorie}\")\n",
    "        continue\n",
    "    \n",
    "    # Create layer name\n",
    "    layer_name = f\"cut_{categorie.replace(' ', '_')}\"\n",
    "    \n",
    "    # Create GeoDataFrame for cut areas\n",
    "    cut_area_gdf = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"name\": [f\"Cut areas - {categorie}\"],\n",
    "            \"description\": [f\"Areas removed due to {categorie}\"],\n",
    "            \"categorie\": [categorie]\n",
    "        },\n",
    "        geometry=[category_union],\n",
    "        crs=base_polygon.crs\n",
    "    )\n",
    "      \n",
    "    # Save to GeoPackage\n",
    "    cut_area_gdf.to_file(output_file, layer=layer_name, driver=\"GPKG\", mode='a')\n",
    "    print(f\"Saved cut areas for '{categorie}' as layer '{layer_name}'\")\n",
    "\n",
    "print(f\"\\nAll layers saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d1aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL, SIMPLIFY INCLUDING LAYERS\n",
    "\n",
    "# Simplify and validate all layers (remaining area + cut areas by category)\n",
    "\n",
    "input_file = \"utrecht_cut_with_categories.gpkg\"\n",
    "output_file = \"utrecht_cut_with_categories_simplified_2.gpkg\"\n",
    "\n",
    "# Get all layers from the input GeoPackage\n",
    "layers_df = gpd.list_layers(input_file)\n",
    "print(f\"Found {len(layers_df)} layers\")\n",
    "print(f\"Layer info:\\n{layers_df}\")\n",
    "\n",
    "# Extract layer names from the 'name' column\n",
    "layer_names = layers_df['name'].tolist()\n",
    "print(f\"\\nLayer names: {layer_names}\")\n",
    "\n",
    "for idx, layer_name in enumerate(layer_names):\n",
    "    print(f\"\\n=== Processing layer {idx+1}/{len(layer_names)}: {layer_name} ===\")\n",
    "    \n",
    "    try:\n",
    "        # 1) Read the layer\n",
    "        gdf = gpd.read_file(input_file, layer=layer_name)\n",
    "        \n",
    "        if gdf.empty:\n",
    "            print(f\"Layer {layer_name} is empty, skipping\")\n",
    "            continue\n",
    "        \n",
    "        assert gdf.crs is not None, f\"CRS missing for layer {layer_name}\"\n",
    "        \n",
    "        # 2) Ensure polygonal, valid parts only\n",
    "        gdf[\"geometry\"] = gdf.geometry.apply(shapely.make_valid)\n",
    "        gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.notna()]\n",
    "        \n",
    "        if gdf.empty:\n",
    "            print(f\"No valid geometries in layer {layer_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # 3) Explode geometry collections\n",
    "        gdf = gdf.explode(ignore_index=True)\n",
    "        \n",
    "        # 4) Keep only polygonal types\n",
    "        gdf = gdf[gdf.geom_type.isin([\"Polygon\", \"MultiPolygon\"])]\n",
    "        \n",
    "        if gdf.empty:\n",
    "            print(f\"No polygonal geometries remain for layer {layer_name}\")\n",
    "            continue\n",
    "        \n",
    "        # 5) Union into a single polygonal geometry\n",
    "        poly_union = unary_union(gdf.geometry.values)\n",
    "        \n",
    "        def polygons_only(geom):\n",
    "            if geom.is_empty:\n",
    "                return geom\n",
    "            if isinstance(geom, (Polygon, MultiPolygon)):\n",
    "                return geom\n",
    "            if isinstance(geom, GeometryCollection):\n",
    "                polys = [g for g in geom.geoms if isinstance(g, (Polygon, MultiPolygon))]\n",
    "                if not polys:\n",
    "                    return shapely.geometry.GeometryCollection()\n",
    "                return unary_union(polys)\n",
    "            return shapely.geometry.GeometryCollection()\n",
    "        \n",
    "        poly_union = polygons_only(poly_union)\n",
    "        poly_union = shapely.make_valid(poly_union)\n",
    "        \n",
    "        if poly_union.is_empty:\n",
    "            print(f\"Empty geometry after processing for layer {layer_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        if isinstance(poly_union, Polygon):\n",
    "            poly_union = MultiPolygon([poly_union])\n",
    "        \n",
    "        # 6) Remove tiny slivers (< 1 m²)\n",
    "        tmp = gpd.GeoDataFrame(geometry=[poly_union], crs=gdf.crs).explode(index_parts=False)\n",
    "        tmp = tmp[tmp.area > 1.0]  # keep > 1 m²\n",
    "        \n",
    "        if tmp.empty:\n",
    "            print(f\"All polygons too small for layer {layer_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        poly_union = unary_union(tmp.geometry.values)\n",
    "        \n",
    "        if isinstance(poly_union, Polygon):\n",
    "            poly_union = MultiPolygon([poly_union])\n",
    "        \n",
    "        # 7) Save the simplified layer\n",
    "        # Get original attributes from first row\n",
    "        attrs = gdf.iloc[0].drop('geometry').to_dict()\n",
    "        \n",
    "        out = gpd.GeoDataFrame(\n",
    "            [attrs],\n",
    "            geometry=[poly_union],\n",
    "            crs=gdf.crs\n",
    "        )\n",
    "        \n",
    "        print(f\"  Geometry type: {out.geom_type.iloc[0]}\")\n",
    "        print(f\"  Area: {out.geometry.iloc[0].area:.2f} sq meters\")\n",
    "        print(f\"  Number of parts: {len(list(out.geometry.iloc[0].geoms)) if hasattr(out.geometry.iloc[0], 'geoms') else 1}\")\n",
    "        \n",
    "        # Save (first layer creates file, others append)\n",
    "        if idx == 0:\n",
    "            out.to_file(output_file, layer=layer_name, driver=\"GPKG\")\n",
    "        else:\n",
    "            out.to_file(output_file, layer=layer_name, driver=\"GPKG\", mode='a')\n",
    "        \n",
    "        print(f\"  ✓ Saved simplified layer '{layer_name}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing layer '{layer_name}': {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"All simplified layers saved to {output_file}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 8) Verify all layers\n",
    "print(\"\\nVerification summary:\")\n",
    "print(\"-\" * 60)\n",
    "try:\n",
    "    layers_df_out = gpd.list_layers(output_file)\n",
    "    for layer_name in layers_df_out['name'].tolist():\n",
    "        chk = gpd.read_file(output_file, layer=layer_name)\n",
    "        print(f\"\\nLayer: {layer_name}\")\n",
    "        print(f\"  Records: {len(chk)}\")\n",
    "        print(f\"  Geometry type: {chk.geom_type.iloc[0]}\")\n",
    "        print(f\"  CRS: {chk.crs}\")\n",
    "        print(f\"  Total area: {chk.geometry.iloc[0].area:.2f} sq meters\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during verification: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
